# ğŸš€ SACFormer: Soft Actor-Critic with Transformers in MuJoCo & Gymnasium
A modern Reinforcement Learning (RL) framework using Soft Actor-Critic (SAC) with Transformer-based policies for MuJoCo physics simulations. Designed for multi-GPU training, efficient experience replay, and scalable parallel execution.

# ğŸ“Œ Features
    âœ… SAC-Based RL â€“ Uses Soft Actor-Critic for optimal policy learning.
    âœ… Transformer-Based Actor â€“ Replaces MLP with self-attention for better sequential decision-making.
    âœ… MuJoCo + Gymnasium â€“ High-quality physics simulations for realistic training.
    âœ… Multi-GPU Training â€“ Supports distributed RL training via Ray RLlib.
    âœ… Vectorized Environments â€“ Uses Stable-Baselines3 VecEnv for fast rollouts.
    âœ… Prioritized Experience Replay (PER) â€“ Smart sampling for faster, efficient learning.
    âœ… TensorBoard & Weights & Biases (WandB) Integration â€“ Real-time monitoring & logging.

# ğŸ’¾ Installation Guide
Follow these steps to clone, install dependencies, and run the project.

## Step 1: Clone the Repository
```bash
git clone https://github.com/Riffe007/SACFormer.git
cd SACFormer
```
## Step 2: Create a Virtual Environment (Optional but Recommended)
```bash
python -m venv sac-env
source sac-env/bin/activate  # MacOS/Linux
sac-env\Scripts\activate     # Windows
```
## Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```
Alternatively, install core dependencies manually:

```bash
pip install torch stable-baselines3[extra] gymnasium mujoco ray rllib numpy wandb
```
## Step 4: Verify Installation
Check that dependencies installed correctly:

```bash
python -c "import torch; import gymnasium; import mujoco; import stable_baselines3; print('âœ… Installation successful!')"
```
# ğŸ› ï¸ Usage: Running the Training
Train SAC with Transformers on HalfCheetah
```bash
python training_scripts/sac_train.py
```
Run Parallel Training with Ray RLlib
```bash
python training_scripts/sac_ray_train.py
```
Perform Automated Hyperparameter Tuning
```bash
python training_scripts/sac_hyperparam_search.py
```
# ğŸ“‚ Directory Structure

```plaintext
SAC-Transformer-RL/
â”‚â”€â”€ agents/                     # SAC Agent & Models
â”‚   â”œâ”€â”€ policy.py                # Transformer-based Actor-Critic
â”‚   â”œâ”€â”€ sac_agent.py             # SAC Trainer
â”‚   â”œâ”€â”€ q_network.py             # Twin Q-Networks
â”‚   â”œâ”€â”€ replay_buffer.py         # PER Experience Replay
â”‚   â”œâ”€â”€ train.py                 # Training Script
â”‚   â”œâ”€â”€ evaluation.py            # Evaluation Script
â”‚
â”‚â”€â”€ environments/                # Environment Setup
â”‚   â”œâ”€â”€ gym_env.py               # Gymnasium Wrapper
â”‚   â”œâ”€â”€ vec_env.py               # Parallelized VecEnv
â”‚   â”œâ”€â”€ mujoco_envs.py           # MuJoCo Integration
â”‚
â”‚â”€â”€ utils/                       # Helper Functions
â”‚   â”œâ”€â”€ logging.py               # TensorBoard & WandB Integration
â”‚   â”œâ”€â”€ config.py                # Hyperparameter Storage
â”‚   â”œâ”€â”€ utils.py                 # Common Utilities
â”‚
â”‚â”€â”€ training_scripts/            # Training Variants
â”‚   â”œâ”€â”€ sac_train.py             # Standard SAC Training
â”‚   â”œâ”€â”€ sac_ray_train.py         # Multi-GPU Training with Ray
â”‚   â”œâ”€â”€ sac_hyperparam_search.py # Auto Hyperparameter Tuning
â”‚
â”‚â”€â”€ logs/                        # Training Logs
â”‚
â”‚â”€â”€ models/                      # Saved Models
â”‚   â”œâ”€â”€ sac_halfcheetah.pth      # SAC Weights
â”‚
â”‚â”€â”€ README.md                    # Documentation
â”‚â”€â”€ requirements.txt              # Dependencies
â”‚â”€â”€ train.py                      # Main Entry Point
```

```markdown
## ğŸ”¬ Improvements Over Previous Implementations

| **Old ARS Project**             | **New SAC-Transformer Project**         |
|----------------------------------|-----------------------------------------|
| Augmented Random Search (ARS)   | âœ… Soft Actor-Critic (SAC)              |
| PyBullet Environments           | âœ… MuJoCo + Gymnasium                   |
| No GPU Support                  | âœ… Multi-GPU (Ray RLlib)                |
| Manual Policy Updates (NumPy)   | âœ… Transformer-Based Actor-Critic       |
| No Parallelization              | âœ… Vectorized Environments (VecEnv)     |
| Basic Replay Buffer             | âœ… Prioritized Experience Replay (PER)  |
| Minimal Logging                 | âœ… TensorBoard + WandB                  |
```
## ğŸ”® Future Plans
## ğŸš€ Upcoming Enhancements:

    ğŸ”¹ Meta-RL Support (Memory-Augmented Networks).
    ğŸ”¹ GATO & Decision Transformer experiments.
    ğŸ”¹ Optimized JAX version for TPU acceleration.
    ğŸ”¹ Multi-Agent RL support.
# ğŸ’¡ Why This is the Best Modern RL Setup
    âœ… State-of-the-Art RL (SAC + Transformers)
    âœ… High-Quality Physics (MuJoCo)
    âœ… Parallel Training (Ray RLlib)
    âœ… Production-Ready Code

# ğŸ”¥ This framework is built for RL research and real-world deployment. ğŸš€

# ğŸ¤ Contributing
Contributions are welcome! If youâ€™d like to improve the repo:

1. Fork the project
2. Create a new branch
3. Commit your changes
4. Push to your branch and submit a PR
# ğŸ“œ License
This project is licensed under the MIT License.

# ğŸ“© Contact & Support
For issues, open a GitHub issue or reach out via email:
âœ‰ï¸ techavenger83@gmail.com

